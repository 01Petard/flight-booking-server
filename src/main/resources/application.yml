server:
  port: 8080
  servlet:
    encoding:
      charset: UTF-8
      enabled: true
      force: true

spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      embedding:
        model: shaw/dmeta-embedding-zh-small:latest
      chat:
        model: qwen3:0.6b
        options:
          temperature: 0.8
    openai:
      api-key: sk-0c855b1a08634e48b9b5680785f9d8f7
      base-url: https://api.deepseek.com/chat/completions
      chat:
        options:
          model: deepseek-chat
          temperature: 0.8
    mcp:
      client:
        enabled: true
        name: my-mcp-client
        version: 1.0.0
        request-timeout: 30s
        type: ASYNC  # or SYNC
        sse:
          connections:
            server1:
              url: http://localhost:8088

# 调试日志
logging:
  level:
    io:
      modelcontextprotocol:
        client: DEBUG
        spec: DEBUG